---
title: Tensorflow实战Google-第四章深层神经网络知识点
tags: 新建,模板,小书匠
grammar_cjkRuby: true
---

1.深层神经网络的两个重要参数：多层和非线性
	
* 非线性：
	* 相对于以往的神经网络，由于没有使用激活函数，构造出来的函数往往形如：w1x1+w2x2+...+wnxn + b = 0,很显然这种函数只能模拟线性分割。即只能通过直线来划分，一旦分割面是一个圆形，通过这种方式只能尽可能的得到一个多棱角保卫面，而不能拟合成圆形，存在很大的误差。
	* 细想一下，如果我们换一种权重作用方式，比如将w1x1换为x1^w1 或者 w1*e^x1,很显然这种指数函数作用的结果是一种弯曲状态，就能够拟合上面所说的圆形。但是，目前我们采用的方式是直接在输出层外加上一层激活函数（弯曲函数），就能够实现这种方式。激活函数一般有sigmoid、指数函数等，不同的函数作用效果也不一样。

* 多层
	* 还是相对于之前的神经网络，由于之前的神经网络没有隐藏层，相当于只有一层权重作用在输入变量上面，这样，w1x1+w2x2+...+wnxn + b = 0函数作用下，无论是几维空间，输出的结果总是为一条直线。
	* 考虑下简单地二维空间，比如进行异或运算。这种方式显然不能够通过一条直线就能够分成两类。再到多维，那将更不可能，一条直线只能分两类，多个类就无法实行。
	* 现在我们想想，既然一层能画一条直线，那我多画几条直线，然后将这两条直线组合一下不就可以了吗？确实是这样，比如进行异或运算，加上一个隐藏层，隐藏层节点为4，这输入到这四个节点的都负责自己的一部分划分，分别划分四个点区域，这样，输出处理时将这四个区域进行组合，就是整个完整的区域。

2.损失函数

* 损失函数度量了训练结果和实际结果之间的一种差别，通过这种差别大小来调整神经网络的参数，以此达到优化神经网络的目的。
* 经典损失函数
分类问题的损失函数一般使用交叉熵配合softmax回归；回归问题由于是连续的，一般只有一个输出节点，所以损失函数使用的是均方误差MSE。
	* 损失函数的计算方式有很多，不同的领域都有各自最优化的方式。经典损失函数就是分类问题和回归问题经常使用到的损失函数。
	* 经典损失函数是一种对训练输出值和实际值相似度的度量，值越小，相似度越大，更准确的解释：经典损失函数（交叉熵）刻画了两个分布概率之间的距离。具体为什么好用，实用就行，暂时不管。
	* 公式：H(p,q)=−∑p(x)logq(x)，这里的p代表真确答案，q代表预测值
	* 显然∑q(x)=1，即概率和等于1。因此，我们需要将输出转化为概率类型。一般而言，我们可以直接计算输出值在整个输出中出现的概率作为计算值，这里我们使用了softmax函数。
	* softmax回归函数，是将神经网络的输出结果变成概率分布，softmax(yi)=yi'=e^yi/∑e^yj
	* 均方误差函数：MSE(y,y′)=∑(yi−y’i)^2/n
	* 其他损失函数：不同问题不同对待

3. 神经网络优化-BP算法和梯度下降算法

* 梯度下降算法：
	* 梯度的反方向是函数下降最快的方向，通过这个方式计算，就能够使得函数向着极小值方向迭代，从而达到训练的目的。
	* **学习率**:通过在梯度下降值上加上一个学习率权重，来控制下降的幅度/步长，即控制下降速度的快慢。
	* 几个缺点：
		1.只是局部最优解不是全局最优解
		2.计算时间长-由于损失函数计算的是所有训练数据上的损失和，所以计算量大
		3.为了加快梯度下降，我们可以采用随机梯度下降或者小批量随机下降

4. 进一步优化

* 学习率的优化：在训练初期，差别往往很大，所以这个时候学习率相对较大能够加快训练的速度；但是随着训练的深入，差别减小，为了防止下降跨度太大导致越界，需要降低学习率；这个时候就可以对学习了本进行指数衰减。
* 过拟合问题：样本不足、样本有噪声、模型结构过于复杂都将导致模型过拟合。
	* 正则化：为了避免模型复杂导致的过拟合，我们引入了一个思想，即在损失函数中引入/加入衡量模型复杂度的指标，r*R(w),r为正则化系数，R(w)为描述的是模型参数的大小，通过之中方式限制模型参数的大小来限制模型的复杂度。L1、L2正则化
* 滑动平均模型：为了使得模型更加健壮，即更加稳定。我们使用了滑动平均模型。
	* 这种模型通过在损失函数中加入一个衰减率decay来，缓冲模型参数变量的变化程度，即不让他变化过大，能走10步的，只让它走一步。
	* 衰减变量：上面这种方式训练速度有点慢，为了让训练初期快，比如走9步，有引入了参数衰减变量，通过训练次数来控制滑动平均的步长大小，越到后期步长越慢。
