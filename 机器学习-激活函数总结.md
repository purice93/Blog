---
title: 机器学习-激活函数总结
tags: 新建,模板,小书匠
grammar_cjkRuby: true
---
>激活函数就相当于人体的神经元，它的主要作用就是对输入的信号进行转换，比如将疼痛转化为疼痛信号。但是疼痛的来源有很多种，也就是可能来自于很多的上一个层的神经细胞，比如敲打、拉扯、针刺等等，但是归根结底，它就是一个痛。但是为什么会存在这么多的激活函数？原因在于，当前的计算机并不能很好地模拟人体的神经元，比如，敲打可以加重疼痛，但是药物也可以缓解疼痛，但是对于Sigmoid函数，它的输出并不是以0为中心，所以只能进行加性计算。

**激活函数的几个改进点：**
激活函数的不同主要在于，对于不同的模型，选择更合适的函数来模拟人体神经网络的结构
1. 梯度消失
这个问题主要是在深度学习之后出现的，深度学习之前主要使用的是sigmoid。但是这个函数由于靠近0-1两端的时候，梯度几乎接近于零。即，随着网络的不断训练，输出值会越来越靠近0-1两端，此时，梯度将接近于零，导致方向传播基本上不会发生梯度改变，即神经元饱和，就如同神经麻木一样，此时的神经元将没有任何感觉能力。
2. 以零为中心-zero-centered
意思就是0均值，但是对于这个问题，网上的一直没有很好的解释清楚，我这里说说我的理解
神经网络并不是只有一个权重w，而是由一群权重w组成，这些权重都是统一进行梯度计算，也就是w只能一起加上或者减去梯度值，这样就导致一个问题，比如对一组权重w，如果最优点的权重比一部分w大，比另一部分要小，那么由于只能统一加减计算，所以，不得不先更新一部分，保持另一部分为0,；然后再更新一部分，这样的效率将会很低，很显然，如果正负都可以计算，完全可以一步到位。计算步骤如下图：
![最优值在第四象限，所以有正有负][1]
参考链接：[https://zhuanlan.zhihu.com/p/25110450][2]
3. 计算成本
深度神经网络需要进行大量的计算，之前的激活函数都是指数或者倒数函数，计算复杂，将大大增加计算量，因此，为了简化计算量，现在开始大量使用RELU函数，即max(x,0)

常用激活函数及其导数：
![enter description here][3]


----------
### 激活函数分类

1. tf.nn.relu
修正线性单元(Rectified linear unit) 
![enter description here][4]
![enter description here][5]

**ReLU也有几个需要特别注意的问题：**
* ReLU的输出不是zero-centered
* Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。

尽管存在这两个问题，ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试！

----------


2. tf.nn.relu6
3. tf.nn.crelu
4. tf.nn.elu
指数线性单元(Exponential Linear Units)
![其中a>0.][6]
![enter description here][7]

ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，以及：
* 不会有Dead ReLU问题
* 输出的均值接近0，zero-centered

它的一个小问题在于计算量稍大。类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据ELU总是优于ReLU。

----------


5. tf.nn.selu
6. tf.nn.softplus
![][8] 
![enter description here][9]

----------
7. tf.nn.softsign
![enter description here][10]
![enter description here][11]

----------
8. tf.sigmoid
![enter description here][12]
![enter description here][13]

----------
9. tf.tanh
![enter description here][14]


----------
补充：
1. Leaky ReLU函数
![enter description here][15]
![enter description here][16]
人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为0.01x而非0。另外一种直观的想法是基于参数的方法，即Parametric ReLU:f(x) = \max(\alpha x, x)，其中\alpha
可由back propagation学出来。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。


----------
2. SoftMax
![enter description here][17]
这个公式一般用于多分类问题，比如手写数字识别中，有10个分类，我们最后的输出并不是10个数中的某一个，而是每个数字都占有一定的权重，这里的意思就是权重占比。权重越大的被取到的可能性越高。
>一个别人的例子：SVM只选自己喜欢的男神，Softmax把所有备胎全部拉出来评分，最后还归一化一下
参考：[enter description here][18]


**小结**
建议使用ReLU函数，但是要注意初始化和learning rate的设置；可以尝试使用Leaky ReLU或ELU函数；不建议使用tanh，尤其是sigmoid函数。


**关键词**
* 梯度消失
例如sigmoid的函数，它的导数的取值范围是(0, 0.25]，也就是说对于导数中的每一个元素，我们都有 ![enter description here][19] ，![enter description here][20]  ，小于1的数乘在一起，必然是越乘越小的。这才仅仅是3层，如果10层的话， 根据乘法计算，第10层的误差相对第一层卷积的参数的梯度将是一个非常小的值，接近于0，这就是所谓的“梯度消失”。

参考：[https://zhuanlan.zhihu.com/p/25110450][21]


  [1]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534751862454.jpg
  [2]: https://zhuanlan.zhihu.com/p/25110450
  [3]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534748317099.jpg
  [4]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534747520636.jpg
  [5]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534747542676.jpg
  [6]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534747797374.jpg
  [7]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534747802827.jpg
  [8]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534747856489.jpg
  [9]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534747887661.jpg
  [10]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534747896647.jpg
  [11]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534748169701.jpg
  [12]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534748169701.jpg
  [13]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534748186367.jpg
  [14]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534748229365.jpg
  [15]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534753103917.jpg
  [16]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534753107202.jpg
  [17]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534753870200.jpg
  [18]: https://www.zhihu.com/question/23765351
  [19]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534753456388.jpg
  [20]: http://osiy4s0ad.bkt.clouddn.com/soundblog/1534753462953.jpg
  [21]: https://zhuanlan.zhihu.com/p/25110450
